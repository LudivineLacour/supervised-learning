{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "- Scaling between 0 and 1 because we have a lot of dummies and boolean values\n",
    "- Feature engineering using filter and RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "pd.set_option('max_columns',70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/airbnb_paris_clean_dummies.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_dummies = pd.read_csv('../data/airbnb_paris_clean_wo_dummies.csv')\n",
    "print(df_wo_dummies.shape)\n",
    "df_wo_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking duplicates and drop them\n",
    "\n",
    "print(\"duplicates found in dummies csv:\",df.duplicated().sum())\n",
    "df = df.drop_duplicates()\n",
    "print(\"shape df with dummies\",df.shape,'\\n')\n",
    "\n",
    "print(\"duplicates found in other csv:\",df_wo_dummies.duplicated().sum())\n",
    "df_wo_dummies = df_wo_dummies.drop_duplicates()\n",
    "print(\"shape df without dummies\",df_wo_dummies.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________\n",
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_scale = df[['time_since_host','number_of_reviews','availability_365','extra_people','guests_included',\n",
    "'bedrooms','bathrooms','accommodates']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking distribution and value range before scaling \n",
    "\n",
    "fig,axs=plt.subplots(2,4,figsize=(17,8))\n",
    "\n",
    "for i in range(minmax_scale.shape[1]):\n",
    "    ax = axs[i//4,i%4]\n",
    "    sns.distplot(minmax_scale.iloc[:,i],ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating  copy of initial dataframe in case of mistake\n",
    "\n",
    "df_scaled = df.copy()\n",
    "df_scaled_wo_dummies = df_wo_dummies.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling for dataframe with dummies \n",
    "\n",
    "    # Scaling the data using MinMax method to put all data between 0,1 because we have many columns with dummies\n",
    "    # Also MixMax Scale is recommanded for algos distance-based such as KNN or XGboost\n",
    "\n",
    "for i in range(minmax_scale.columns.shape[0]):\n",
    "    df_scaled[minmax_scale.columns[i]] = (df_scaled[minmax_scale.columns[i]] - df_scaled[minmax_scale.columns[i]].min())/(df_scaled[minmax_scale.columns[i]].max()- df_scaled[minmax_scale.columns[i]].min())\n",
    "\n",
    "    # Scaling price diving by 100 \n",
    "\n",
    "df_scaled.price = df_scaled.price/100\n",
    "\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling data for dataframe without dummies\n",
    "\n",
    "    # Scaling the data using MinMax method for numerical columns\n",
    "\n",
    "for i in range(minmax_scale.columns.shape[0]):\n",
    "    df_scaled_wo_dummies[minmax_scale.columns[i]] = (df_scaled_wo_dummies[minmax_scale.columns[i]] - df_scaled_wo_dummies[minmax_scale.columns[i]].min())/(df_scaled_wo_dummies[minmax_scale.columns[i]].max()-df_scaled_wo_dummies[minmax_scale.columns[i]].min())\n",
    "\n",
    "    \n",
    "    # Scaling price diving by 100 \n",
    "df_scaled_wo_dummies.price = df_scaled_wo_dummies.price/100\n",
    "\n",
    "df_scaled_wo_dummies.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________\n",
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.feature_selection import SelectKBest, f_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering using Filter method with f-regression for Linear Regression model \n",
    "# and mutual_info_regression for KNN and XGboost models\n",
    "\n",
    "X = df.drop('price',axis=1)\n",
    "y = df.price\n",
    "\n",
    "X_scaled = df_scaled.drop('price',axis=1)\n",
    "y_scaled = df_scaled.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_freg = SelectKBest(f_regression)\n",
    "X_new_freg = selection_freg.fit_transform(X_scaled,y_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_freg = pd.DataFrame(X_new_freg)\n",
    "print(X_new_freg.shape)\n",
    "X_new_freg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true,y_pred):\n",
    "    if y_true.any() == 0:\n",
    "        return \"dividing by 0 is impossible\"\n",
    "    else:\n",
    "        return np.mean(np.abs((y_true-y_pred)/y_pred))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test using RFE method\n",
    "\n",
    "estimator = LinearRegression()\n",
    "\n",
    "# build RFE on non-scaled df\n",
    "selection = RFE(estimator, 10)\n",
    "selection.fit(X,y)\n",
    "print(X.columns[selection.support_])\n",
    "y_pred_rfe = selection.predict(X)\n",
    "print('R2 using RFE method without scale:',r2_score(y,y_pred_rfe))\n",
    "print('MAPE:',mape(y,y_pred_rfe),'\\n')\n",
    "\n",
    "# build RFE using scaled df\n",
    "selection2 = RFE(estimator, 10)\n",
    "selection2.fit(X_scaled,y_scaled)\n",
    "print(X_scaled.columns[selection2.support_])\n",
    "y_pred_rfe2 = selection2.predict(X_scaled)\n",
    "print('R2 using RFE method:',r2_score(y_scaled,y_pred_rfe2))\n",
    "print('MAPE:',mape(y_scaled,y_pred_rfe2),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a for loop to find the best number of columns\n",
    "r_score = []\n",
    "mape_list = []\n",
    "\n",
    "for i in range(2,30):\n",
    "    selection = RFE(estimator, i)\n",
    "    selection.fit(X_scaled,y_scaled)\n",
    "    print(\"Number of columns\",i)\n",
    "    #print(X_scaled.columns[selection.support_])\n",
    "    y_pred_rfe = selection.predict(X_scaled)\n",
    "    r_score.append(r2_score(y_scaled,y_pred_rfe))\n",
    "    #print('R2 using RFE method:',r2_score(y_scaled,y_pred_rfe))\n",
    "    mape_list.append(mape(y_scaled,y_pred_rfe))\n",
    "    #print('MAPE:',mape(y_scaled,y_pred_rfe),'\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing graph to show the result of model performance depending on the number of columns\n",
    "\n",
    "nb_cols = list(range(2,30))\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(14,5))\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.plot(nb_cols, r_score, linestyle='-', marker='o', color=color)\n",
    "y0, y1 = ax1.get_ylim()\n",
    "ax1.vlines(x=23,ymin=y0,ymax=y1, linestyle='dashed', label='Best Shape = [20,23]')\n",
    "ax1.vlines(x=20,ymin=y0,ymax=y1, linestyle='dashed')\n",
    "ax1.set_xlabel('Number of columns')\n",
    "ax1.set_ylabel('R^2 Score', color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.legend()\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('MAPE', color=color)\n",
    "ax2.plot(nb_cols, mape_list, linestyle='-', marker='o', color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.suptitle('Model performance depending on the number of columns')\n",
    "#fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.savefig('../img/rfe_method_model_performance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building best selection \n",
    "\n",
    "best_selection = RFE(estimator, 20)\n",
    "best_selection.fit(X_scaled,y_scaled)\n",
    "\n",
    "\n",
    "X_new_RFE = X_scaled[X_scaled.columns[best_selection.support_]]\n",
    "X_new_RFE.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the performance of model depending on columns used (w/ and w/o filter method)\n",
    "\n",
    "# Use df non-scaled without any feature engineering method (so all columns)\n",
    "lin_model3 = LinearRegression()\n",
    "lin_model_fitted3 = lin_model3.fit(X,y)\n",
    "y_pred_lin3 = lin_model_fitted3.predict(X)\n",
    "print('R2 w/o feature engineering methods nor scaling:',r2_score(y,y_pred_lin3))\n",
    "print('MAPE:',mape(y,y_pred_lin3),'\\n')\n",
    "\n",
    "# Use df scaled without any feature engineering method (so all columns)\n",
    "lin_model2 = LinearRegression()\n",
    "lin_model_fitted2 = lin_model2.fit(X_scaled,y_scaled)\n",
    "y_pred_lin2 = lin_model_fitted2.predict(X_scaled)\n",
    "print('R2 w/o feature engineering methods:',r2_score(y_scaled,y_pred_lin2))\n",
    "print('MAPE:',mape(y_scaled,y_pred_lin2),'\\n')\n",
    "\n",
    "# Use filter method\n",
    "lin_model = LinearRegression()\n",
    "lin_model_fitted = lin_model.fit(X_new_freg,y_scaled)\n",
    "y_pred_lin = lin_model_fitted.predict(X_new_freg)\n",
    "print('R2 using filter method:',r2_score(y_scaled,y_pred_lin))\n",
    "print('MAPE:',mape(y_scaled,y_pred_lin),'\\n')\n",
    "\n",
    "\n",
    "# Use RFE method\n",
    "lin_model4 = LinearRegression()\n",
    "lin_model_fitted4 = lin_model4.fit(X_new_RFE,y_scaled)\n",
    "y_pred_lin4 = lin_model_fitted4.predict(X_new_RFE)\n",
    "print('R2 using RFE method:',r2_score(y_scaled,y_pred_lin4))\n",
    "print('MAPE:',mape(y_scaled,y_pred_lin4))\n",
    "\n",
    "\n",
    "# Not using RMSLE here because I got an error on negative predicted values \n",
    "# code: print('RMSLE:',(mean_squared_log_error(y_scaled,abs(y_pred_lin4))**0.5),'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lin4[y_pred_lin4<0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "\n",
    "- using scaling is better for linear regression especially if using RFE method for feature engineering\n",
    "- filter method seems to have better performance than RFE (R2 is better) but errors are the same\n",
    "- In any case it is better to use feature engineering methods because errors are smaller\n",
    "\n",
    "**Possible improvements:**\n",
    "- test Sequential Selection to find out the best number of features needed and compare results with other methods\n",
    "- run PCA\n",
    "\n",
    "**Next steps:**\n",
    "- ~~test with different number of features~~\n",
    "- ~~export new csv with scaled data and RFE method~~\n",
    "- ~~run LinearRegression, KNN and RandomForest models to compare results using evaluation metrics for regression~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the new csv with feature engineering methods\n",
    "\n",
    "feat_df = df_scaled[list(df_scaled[X_new_RFE.columns])+['price']]\n",
    "print(feat_df.shape)\n",
    "\n",
    "feat_df.to_csv('../data/airbnb_paris_clean_RFE.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the new scaled csv without dummies \n",
    "\n",
    "df_scaled_wo_dummies.to_csv('../data/airbnb_paris_clean_wo_dummies_feat.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
