{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling using supervised-learning\n",
    "\n",
    "The end goal is to predict the price per night of a airbnb location depending on several features. \n",
    "\n",
    "We will use the dataset cleaned and preprocessed for modelling (scaling and RFE feature engineering methods). \n",
    "\n",
    "Models to build and compare:\n",
    "- Linear Regression\n",
    "- KNN\n",
    "- RandomForest\n",
    "\n",
    "**Steps:**\n",
    "- Build train/test sample data \n",
    "- Build models\n",
    "- Get evaluation metrics for each models\n",
    "- Compare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/airbnb_paris_clean_RFE.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________\n",
    "### Models set-up & train/test sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true,y_pred):\n",
    "    if y_true.any() == 0:\n",
    "        return \"dividing by 0 is impossible\"\n",
    "    else:\n",
    "        return np.mean(np.abs((y_true-y_pred)/y_pred))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('price', axis=1)\n",
    "y = df.price\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "\n",
    "# Creating several samples to test overfitting of models\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X, y, test_size=0.3, random_state=15)\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________\n",
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the model\n",
    "model_lin = LinearRegression()\n",
    "model_lin.fit(X_train, y_train)\n",
    "print('R^2 score for X_train:', model_lin.score(X_train, y_train),'\\n')\n",
    "\n",
    "# predicting values\n",
    "y_pred_lin = model_lin.predict(X_test)\n",
    "\n",
    "# checking evaluation metrics\n",
    "print('R^2 score for X_test:',r2_score(y_test,y_pred_lin))\n",
    "print('MSE:', mean_squared_error(y_test,y_pred_lin))\n",
    "print('RMSE:', mean_squared_error(y_test,y_pred_lin, squared=False))\n",
    "print('MAPE:',mape(y_test,y_pred_lin))\n",
    "\n",
    "# Can't use RMSLE because of negavite predicted values\n",
    "# print('RMSLE:',(mean_squared_log_error(y_test,abs(y_pred_lin))**0.5),'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_y = y_test*100\n",
    "unscaled_y_pred = y_pred_lin*100\n",
    "\n",
    "# checking evaluation metrics\n",
    "print('R^2 score for X_test:',r2_score(unscaled_y,unscaled_y_pred))\n",
    "print('MSE:', mean_squared_error(unscaled_y,unscaled_y_pred))\n",
    "print('RMSE:', mean_squared_error(unscaled_y,unscaled_y_pred, squared=False))\n",
    "print('MAPE:',mape(unscaled_y,unscaled_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lin[y_pred_lin<0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking predictions\n",
    "plt.scatter(y_test,y_pred_lin)\n",
    "plt.title('Prediction Linearity')\n",
    "plt.show()\n",
    "\n",
    "# Checking residuals\n",
    "resid=y_test-y_pred_lin\n",
    "\n",
    "sns.distplot(resid)\n",
    "plt.title('Distribution of residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________\n",
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building model\n",
    "model_knn = KNeighborsRegressor(n_neighbors=3)\n",
    "model_knn.fit(X_train, y_train)\n",
    "print(\"Sample 1: X_train, X_test\")\n",
    "print('R^2 score for X_train:', model_knn.score(X_train, y_train),'\\n')\n",
    "\n",
    "# predicting values\n",
    "y_pred_knn = model_knn.predict(X_test)\n",
    "\n",
    "# checking evaluation metrics\n",
    "print('R^2 score for X_test:', model_knn.score(X_test, y_test))\n",
    "print('MSE:', mean_squared_error(y_test,y_pred_knn))\n",
    "print('RMSE:', mean_squared_error(y_test,y_pred_knn, squared=False))\n",
    "print('MAPE:',mape(y_test,y_pred_knn))\n",
    "print('RMSLE:',(mean_squared_log_error(y_test, y_pred_knn)**0.5),'\\n')\n",
    "\n",
    "\n",
    "## Testing with other samples\n",
    "\n",
    "# building model\n",
    "model_knn_2 = KNeighborsRegressor(n_neighbors=3)\n",
    "model_knn_2.fit(X_train_2, y_train_2)\n",
    "print(\"Sample 2: X_train_2, X_test_2\")\n",
    "print('R^2 score for X_train:', model_knn_2.score(X_train_2, y_train_2),'\\n')\n",
    "\n",
    "# predicting values\n",
    "y_pred_knn_2 = model_knn_2.predict(X_test_2)\n",
    "\n",
    "# checking evaluation metrics\n",
    "print('R^2 score for X_test:', model_knn_2.score(X_test_2, y_test_2))\n",
    "print('MSE:', mean_squared_error(y_test_2,y_pred_knn_2))\n",
    "print('RMSE:', mean_squared_error(y_test_2,y_pred_knn_2, squared=False))\n",
    "print('MAPE:',mape(y_test_2,y_pred_knn_2))\n",
    "print('RMSLE:',(mean_squared_log_error(y_test_2, y_pred_knn_2)**0.5),'\\n')\n",
    "\n",
    "\n",
    "## Testing with other samples\n",
    "\n",
    "# building model\n",
    "model_knn_3 = KNeighborsRegressor(n_neighbors=3)\n",
    "model_knn_3.fit(X_train_3, y_train_3)\n",
    "print(\"Sample 3: X_train_3, X_test_3\")\n",
    "print('R^2 score for X_train:', model_knn_3.score(X_train_3, y_train_3),'\\n')\n",
    "\n",
    "# predicting values\n",
    "y_pred_knn_3 = model_knn_3.predict(X_test_3)\n",
    "\n",
    "# checking evaluation metrics\n",
    "print('R^2 score for X_test:', model_knn_3.score(X_test_3, y_test_3))\n",
    "print('MSE:', mean_squared_error(y_test_3,y_pred_knn_3))\n",
    "print('RMSE:', mean_squared_error(y_test_3,y_pred_knn_3, squared=False))\n",
    "print('MAPE:',mape(y_test_3,y_pred_knn_3))\n",
    "print('RMSLE:',(mean_squared_log_error(y_test_3, y_pred_knn_3)**0.5),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a for loop to find the best n_neighbors [WARNING BEFORE RUNNING - IT TAKES TIME]\n",
    "\n",
    "r_score_train = [] # Storing results in list\n",
    "r_score_test = []\n",
    "rmse_test = []\n",
    "\n",
    "for i in range(3,21,2):\n",
    "    # building model\n",
    "    model_knn = KNeighborsRegressor(n_neighbors=i)\n",
    "    model_knn.fit(X_train, y_train)\n",
    "    print('Number of neighbors:',i)\n",
    "    print('R^2 score for X_train:', model_knn.score(X_train, y_train),'\\n')\n",
    "    r_score_train.append(model_knn.score(X_train, y_train))\n",
    "    \n",
    "    # predicting values\n",
    "    y_pred_knn = model_knn.predict(X_test)\n",
    "\n",
    "    # checking evaluation metrics\n",
    "    r_score_test.append(model_knn.score(X_test, y_test))\n",
    "    print('R^2 score for X_test:', model_knn.score(X_test, y_test))\n",
    "    print('MSE:', mean_squared_error(y_test,y_pred_knn))\n",
    "    rmse_test.append(mean_squared_error(y_test,y_pred_knn, squared=False))\n",
    "    print('RMSE:', mean_squared_error(y_test,y_pred_knn, squared=False))\n",
    "    print('MAPE:',mape(y_test,y_pred_knn))\n",
    "    print('RMSLE:',(mean_squared_log_error(y_test, y_pred_knn)**0.5),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing graph to show the result model performance depending on the number neighbors\n",
    "\n",
    "nb_neighbors = list(range(3,21,2))\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(14,5))\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.plot(nb_neighbors, r_score_test, linestyle='-', marker='o', color=color)\n",
    "ax1.set_xlabel('Number of neighbors')\n",
    "ax1.set_ylabel('R^2 Score', color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "#y0, y1 = ax1.get_ylim()\n",
    "#ax1.vlines(x=23,ymin=y0,ymax=y1, linestyle='dashed', label='Best Shape = [20,23]')\n",
    "#ax1.vlines(x=20,ymin=y0,ymax=y1, linestyle='dashed')\n",
    "#ax1.legend()\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('RMSE', color=color)\n",
    "ax2.plot(nb_neighbors, rmse_test, linestyle='-', marker='o', color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.suptitle('KNN Model performance depending on neighbors', fontsize=14)\n",
    "plt.savefig('../img/KNN_model_performance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating another visualization to show the overfitting effect \n",
    "\n",
    "plt.plot(nb_neighbors, r_score_train, color='tab:red', label='r_score X_train')\n",
    "plt.plot(nb_neighbors, r_score_test, color='tab:blue', label='r_score X_test')\n",
    "plt.xlabel('Number of neighbors')\n",
    "plt.ylabel('R^2 score')\n",
    "plt.title('Comparision of R^2 score for Train and Test samples', fontsize=14)\n",
    "plt.legend()\n",
    "plt.savefig('../img/comparison_rscore_KNN.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuilding the best model\n",
    "\n",
    "model_knn = KNeighborsRegressor(n_neighbors=19)\n",
    "model_knn.fit(X_train, y_train)\n",
    "print('R^2 score for X_train:', model_knn.score(X_train, y_train),'\\n')\n",
    "\n",
    "# predicting values\n",
    "y_pred_knn = model_knn.predict(X_test)\n",
    "\n",
    "# checking evaluation metrics\n",
    "print('R^2 score for X_test:', model_knn.score(X_test, y_test))\n",
    "print('MSE:', mean_squared_error(y_test,y_pred_knn))\n",
    "print('RMSE:', mean_squared_error(y_test,y_pred_knn, squared=False))\n",
    "print('MAPE:',mape(y_test,y_pred_knn))\n",
    "print('RMSLE:',(mean_squared_log_error(y_test, y_pred_knn)**0.5),'\\n')\n",
    "\n",
    "\n",
    "# Testing with weights on distance\n",
    "    \n",
    "model_knn_w = KNeighborsRegressor(n_neighbors=19, weights='distance')\n",
    "model_knn_w.fit(X_train, y_train)\n",
    "print('R^2 score for X_train with weighted distance:', model_knn_w.score(X_train, y_train),'\\n')\n",
    "\n",
    "# predicting values\n",
    "y_pred_knn_w = model_knn_w.predict(X_test)\n",
    "\n",
    "# checking evaluation metrics\n",
    "print('R^2 score for X_test with weighted distance:', model_knn_w.score(X_test, y_test))\n",
    "print('MSE:', mean_squared_error(y_test,y_pred_knn_w))\n",
    "print('RMSE:', mean_squared_error(y_test,y_pred_knn_w, squared=False))\n",
    "print('MAPE:',mape(y_test,y_pred_knn_w))\n",
    "print('RMSLE:',(mean_squared_log_error(y_test, y_pred_knn_w)**0.5),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_knn[y_pred_knn<0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking predictions\n",
    "plt.scatter(y_test,y_pred_knn)\n",
    "plt.title('Prediction Linearity')\n",
    "plt.show()\n",
    "\n",
    "# Checking residuals\n",
    "resid=y_test-y_pred_knn\n",
    "\n",
    "sns.distplot(resid)\n",
    "plt.title('Distribution of residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________\n",
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Random Forest \n",
    "\n",
    "    # building the model with default estimator\n",
    "    \n",
    "model_rf = RandomForestRegressor()\n",
    "model_rf.fit(X_train, y_train)\n",
    "print(\"R^2 score for X_train:\", model_rf.score(X_train, y_train),'\\n')\n",
    "\n",
    "# predicting values \n",
    "y_pred_rf = model_rf.predict(X_test)\n",
    "\n",
    "# checking evaluation metrics\n",
    "print('R^2 score for X_test:', model_rf.score(X_test, y_test))\n",
    "print('MSE:', mean_squared_error(y_test,y_pred_rf))\n",
    "print('RMSE:', mean_squared_error(y_test,y_pred_rf, squared=False))\n",
    "print('MAPE:',mape(y_test,y_pred_rf))\n",
    "print('RMSLE:',(mean_squared_log_error(y_test, y_pred_rf)**0.5),'\\n')\n",
    "\n",
    "\n",
    "    # building the model with estimator = 1000\n",
    "    \n",
    "model_rf_2 = RandomForestRegressor(n_estimators=1000)\n",
    "model_rf_2.fit(X_train, y_train)\n",
    "print(\"R^2 score for X_train:\", model_rf_2.score(X_train, y_train),'\\n')\n",
    "\n",
    "# predicting values \n",
    "y_pred_rf_2 = model_rf_2.predict(X_test)\n",
    "\n",
    "# checking evaluation metrics\n",
    "print('R^2 score for X_test:', model_rf_2.score(X_test, y_test))\n",
    "print('MSE:', mean_squared_error(y_test,y_pred_rf_2))\n",
    "print('RMSE:', mean_squared_error(y_test,y_pred_rf_2, squared=False))\n",
    "print('MAPE:',mape(y_test,y_pred_rf_2))\n",
    "print('RMSLE:',(mean_squared_log_error(y_test, y_pred_rf_2)**0.5),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test,y_pred_rf)\n",
    "plt.title('Prediction Linearity')\n",
    "plt.show()\n",
    "\n",
    "# Checking residuals\n",
    "\n",
    "resid=y_test-y_pred_rf\n",
    "\n",
    "sns.distplot(resid)\n",
    "plt.title('Distribution of residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv('../data/airbnb_paris_clean_wo_dummies_feat.csv')\n",
    "print(df_original.shape)\n",
    "df_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building samples\n",
    "X_orig = df_original.drop('price',axis=1)\n",
    "y_orig = df_original.price\n",
    "\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X_orig, y_orig, test_size=0.3, random_state=8)\n",
    "\n",
    "# building the model \n",
    "model_rf = RandomForestRegressor()\n",
    "model_rf.fit(X_train_orig, y_train_orig)\n",
    "print(\"R^2 score for X_train:\", model_rf.score(X_train_orig, y_train_orig),'\\n')\n",
    "\n",
    "# predicting values \n",
    "y_pred_rf = model_rf.predict(X_test_orig)\n",
    "\n",
    "# checking evaluation metrics\n",
    "print('R^2 score for X_test:', model_rf.score(X_test_orig, y_test_orig))\n",
    "print('MSE:', mean_squared_error(y_test_orig,y_pred_rf))\n",
    "print('RMSE:', mean_squared_error(y_test_orig,y_pred_rf, squared=False))\n",
    "print('MAPE:',mape(y_test_orig,y_pred_rf))\n",
    "print('RMSLE:',(mean_squared_log_error(y_test_orig, y_pred_rf)**0.5),'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "\n",
    "- At first sight Linear Regression and KNeighbors are the best model because they have almost the same result, although Linear Regression it is not overfitted (from the beginning), but all assumptions aren't met.\n",
    "- In the meantime, the Linear Regression predicted negative values (is it wrong?)\n",
    "- The models seems to be overfitted on the trained sample for KNN and Random Forest models even though I managed to reduce the overfitting in KNN by increasing the number of neighbors\n",
    "- The RMSE score doble from MSE that it is due to outliers still in the dataset\n",
    "\n",
    "**Possible improvements:**\n",
    "- Try Random Forest with dataset without dummies\n",
    "- Try other preprocessing methods (Sequential Selection, PCA and other scaling) to improve the resutls\n",
    "- Try Embedded feature engineering methods (Lasso)\n",
    "\n",
    "**Next Steps:**\n",
    "- ~~Check why there are negative values in prediction values~~\n",
    "- ~~Change scaling method for Price~~\n",
    "- ~~Confirm the overfitting of models~~\n",
    "- Fix Random Forest without dummies\n",
    "- ~~Try Random Forest with an estimator = 1000~~\n",
    "- ~~Build a for loop to find the best n_neighbors for KNN (between 3 and 21)~~\n",
    "- Check assumptions of each models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
